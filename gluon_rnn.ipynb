{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon import nn, rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(path + 'train.txt')\n",
    "        self.valid = self.tokenize(path + 'valid.txt')\n",
    "        self.test = self.tokenize(path + 'test.txt')\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r') as f:\n",
    "            ids = np.zeros((tokens,), dtype='int32')\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "\n",
    "        return mx.nd.array(ids, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(gluon.Block):\n",
    "    \"\"\"A model with an encoder, recurrent layer, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, mode, vocab_size, num_embed, num_hidden,\n",
    "                 num_layers, dropout=0.5, tie_weights=False, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.drop = nn.Dropout(dropout)\n",
    "            self.encoder = nn.Embedding(vocab_size, num_embed,\n",
    "                                        weight_initializer = mx.init.Uniform(0.1))\n",
    "            if mode == 'rnn_relu':\n",
    "                self.rnn = rnn.RNN(num_hidden, num_layers, activation='relu', dropout=dropout,\n",
    "                                   input_size=num_embed)\n",
    "            elif mode == 'rnn_tanh':\n",
    "                self.rnn = rnn.RNN(num_hidden, num_layers, dropout=dropout,\n",
    "                                   input_size=num_embed)\n",
    "            elif mode == 'lstm':\n",
    "                self.rnn = rnn.LSTM(num_hidden, num_layers, dropout=dropout,\n",
    "                                    input_size=num_embed)\n",
    "            elif mode == 'gru':\n",
    "                self.rnn = rnn.GRU(num_hidden, num_layers, dropout=dropout,\n",
    "                                   input_size=num_embed)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid mode %s. Options are rnn_relu, \"\n",
    "                                 \"rnn_tanh, lstm, and gru\"%mode)\n",
    "            if tie_weights:\n",
    "                self.decoder = nn.Dense(vocab_size, in_units = num_hidden,\n",
    "                                        params = self.encoder.params)\n",
    "            else:\n",
    "                self.decoder = nn.Dense(vocab_size, in_units = num_hidden)\n",
    "            self.num_hidden = num_hidden\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        emb = self.drop(self.encoder(inputs))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.reshape((-1, self.num_hidden)))\n",
    "        return decoded, hidden\n",
    "\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_data = './data/ptb.'\n",
    "args_model = 'rnn_relu'\n",
    "args_emsize = 100\n",
    "args_nhid = 100\n",
    "args_nlayers = 2\n",
    "args_lr = 1.0\n",
    "args_clip = 0.2\n",
    "args_epochs = 10\n",
    "args_batch_size = 32\n",
    "args_bptt = 5\n",
    "args_dropout = 0.2\n",
    "args_tied = True\n",
    "args_cuda = 'store_true'\n",
    "args_log_interval = 500\n",
    "args_save = 'model.param'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = mx.gpu(0)\n",
    "corpus = Corpus(args_data)\n",
    "\n",
    "def batchify(data, batch_size):\n",
    "    \"\"\"Reshape data into (num_example, batch_size)\"\"\"\n",
    "    nbatch = data.shape[0] // batch_size\n",
    "    data = data[:nbatch * batch_size]\n",
    "    data = data.reshape((batch_size, nbatch)).T\n",
    "    return data\n",
    "\n",
    "train_data = batchify(corpus.train, args_batch_size).as_in_context(context)\n",
    "val_data = batchify(corpus.valid, args_batch_size).as_in_context(context)\n",
    "test_data = batchify(corpus.test, args_batch_size).as_in_context(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(corpus.dictionary)\n",
    "\n",
    "model = RNNModel(args_model, ntokens, args_emsize, args_nhid,\n",
    "                       args_nlayers, args_dropout, args_tied)\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=context)\n",
    "trainer = gluon.Trainer(model.collect_params(), 'sgd',\n",
    "                        {'learning_rate': args_lr, 'momentum': 0, 'wd': 0})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i):\n",
    "    seq_len = min(args_bptt, source.shape[0] - 1 - i)\n",
    "    data = source[i : i + seq_len]\n",
    "    target = source[i + 1 : i + 1 + seq_len]\n",
    "    return data, target.reshape((-1,))\n",
    "\n",
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [i.detach() for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(data_source):\n",
    "    total_L = 0.0\n",
    "    ntotal = 0\n",
    "    hidden = model.begin_state(func = mx.nd.zeros, batch_size = args_batch_size, ctx=context)\n",
    "    for i in range(0, data_source.shape[0] - 1, args_bptt):\n",
    "        data, target = get_batch(data_source, i)\n",
    "        output, hidden = model(data, hidden)\n",
    "        L = loss(output, target)\n",
    "        total_L += mx.nd.sum(L).asscalar()\n",
    "        ntotal += L.size\n",
    "    return total_L / ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    best_val = float(\"Inf\")\n",
    "    for epoch in range(args_epochs):\n",
    "        total_L = 0.0\n",
    "        start_time = time.time()\n",
    "        hidden = model.begin_state(func = mx.nd.zeros, batch_size = args_batch_size, ctx = context)\n",
    "        for ibatch, i in enumerate(range(0, train_data.shape[0] - 1, args_bptt)):\n",
    "            data, target = get_batch(train_data, i)\n",
    "            hidden = detach(hidden)\n",
    "            with autograd.record():\n",
    "                output, hidden = model(data, hidden)\n",
    "                L = loss(output, target)\n",
    "                L.backward()\n",
    "\n",
    "            grads = [i.grad(context) for i in model.collect_params().values()]\n",
    "            # Here gradient is for the whole batch.\n",
    "            # So we multiply max_norm by batch_size and bptt size to balance it.\n",
    "            gluon.utils.clip_global_norm(grads, args_clip * args_bptt * args_batch_size)\n",
    "\n",
    "            trainer.step(args_batch_size)\n",
    "            total_L += mx.nd.sum(L).asscalar()\n",
    "\n",
    "            if ibatch % args_log_interval == 0 and ibatch > 0:\n",
    "                cur_L = total_L / args_bptt / args_batch_size / args_log_interval\n",
    "                print('[Epoch %d Batch %d] loss %.2f, perplexity %.2f' % (\n",
    "                    epoch + 1, ibatch, cur_L, math.exp(cur_L)))\n",
    "                total_L = 0.0\n",
    "\n",
    "        val_L = eval(val_data)\n",
    "\n",
    "        print('[Epoch %d] time cost %.2fs, validation loss %.2f, validation perplexity %.2f' % (\n",
    "            epoch + 1, time.time() - start_time, val_L, math.exp(val_L)))\n",
    "\n",
    "        if val_L < best_val:\n",
    "            best_val = val_L\n",
    "            test_L = eval(test_data)\n",
    "            model.save_params(args_save)\n",
    "            print('test loss %.2f, test perplexity %.2f' % (test_L, math.exp(test_L)))\n",
    "        else:\n",
    "            args_lr = args_lr * 0.25\n",
    "            trainer._init_optimizer('sgd',\n",
    "                                    {'learning_rate': args_lr,\n",
    "                                     'momentum': 0,\n",
    "                                     'wd': 0})\n",
    "            model.load_params(args_save, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 500] loss 7.19, perplexity 1322.96\n",
      "[Epoch 1 Batch 1000] loss 6.44, perplexity 626.49\n",
      "[Epoch 1 Batch 1500] loss 6.25, perplexity 520.18\n",
      "[Epoch 1 Batch 2000] loss 6.17, perplexity 477.75\n",
      "[Epoch 1 Batch 2500] loss 6.06, perplexity 426.94\n",
      "[Epoch 1 Batch 3000] loss 5.95, perplexity 383.93\n",
      "[Epoch 1 Batch 3500] loss 5.96, perplexity 388.07\n",
      "[Epoch 1 Batch 4000] loss 5.84, perplexity 343.27\n",
      "[Epoch 1 Batch 4500] loss 5.82, perplexity 338.43\n",
      "[Epoch 1 Batch 5000] loss 5.82, perplexity 335.57\n",
      "[Epoch 1 Batch 5500] loss 5.82, perplexity 336.34\n",
      "[Epoch 1] time cost 192.04s, validation loss 5.70, validation perplexity 298.51\n",
      "test loss 5.66, test perplexity 286.52\n",
      "[Epoch 2 Batch 500] loss 5.80, perplexity 328.85\n",
      "[Epoch 2 Batch 1000] loss 5.72, perplexity 306.22\n",
      "[Epoch 2 Batch 1500] loss 5.69, perplexity 296.51\n",
      "[Epoch 2 Batch 2000] loss 5.72, perplexity 304.92\n",
      "[Epoch 2 Batch 2500] loss 5.67, perplexity 289.45\n",
      "[Epoch 2 Batch 3000] loss 5.60, perplexity 269.63\n",
      "[Epoch 2 Batch 3500] loss 5.64, perplexity 281.93\n",
      "[Epoch 2 Batch 4000] loss 5.55, perplexity 257.63\n",
      "[Epoch 2 Batch 4500] loss 5.55, perplexity 256.78\n",
      "[Epoch 2 Batch 5000] loss 5.57, perplexity 263.06\n",
      "[Epoch 2 Batch 5500] loss 5.60, perplexity 270.14\n",
      "[Epoch 2] time cost 191.89s, validation loss 5.49, validation perplexity 241.31\n",
      "test loss 5.45, test perplexity 231.63\n",
      "[Epoch 3 Batch 500] loss 5.60, perplexity 271.18\n",
      "[Epoch 3 Batch 1000] loss 5.54, perplexity 255.87\n",
      "[Epoch 3 Batch 1500] loss 5.52, perplexity 248.92\n",
      "[Epoch 3 Batch 2000] loss 5.57, perplexity 263.08\n",
      "[Epoch 3 Batch 2500] loss 5.53, perplexity 252.58\n",
      "[Epoch 3 Batch 3000] loss 5.47, perplexity 237.62\n",
      "[Epoch 3 Batch 3500] loss 5.52, perplexity 249.66\n",
      "[Epoch 3 Batch 4000] loss 5.44, perplexity 229.88\n",
      "[Epoch 3 Batch 4500] loss 5.43, perplexity 227.45\n",
      "[Epoch 3 Batch 5000] loss 5.47, perplexity 236.44\n",
      "[Epoch 3 Batch 5500] loss 5.50, perplexity 244.20\n",
      "[Epoch 3] time cost 191.80s, validation loss 5.41, validation perplexity 223.90\n",
      "test loss 5.36, test perplexity 213.76\n",
      "[Epoch 4 Batch 500] loss 5.50, perplexity 245.33\n",
      "[Epoch 4 Batch 1000] loss 5.46, perplexity 234.21\n",
      "[Epoch 4 Batch 1500] loss 5.44, perplexity 230.44\n",
      "[Epoch 4 Batch 2000] loss 5.50, perplexity 244.06\n",
      "[Epoch 4 Batch 2500] loss 5.46, perplexity 234.63\n",
      "[Epoch 4 Batch 3000] loss 5.39, perplexity 220.14\n",
      "[Epoch 4 Batch 3500] loss 5.45, perplexity 232.94\n",
      "[Epoch 4 Batch 4000] loss 5.37, perplexity 215.10\n",
      "[Epoch 4 Batch 4500] loss 5.36, perplexity 212.48\n",
      "[Epoch 4 Batch 5000] loss 5.40, perplexity 221.88\n",
      "[Epoch 4 Batch 5500] loss 5.43, perplexity 228.88\n",
      "[Epoch 4] time cost 192.59s, validation loss 5.36, validation perplexity 211.82\n",
      "test loss 5.31, test perplexity 201.64\n",
      "[Epoch 5 Batch 500] loss 5.45, perplexity 232.40\n",
      "[Epoch 5 Batch 1000] loss 5.40, perplexity 221.20\n",
      "[Epoch 5 Batch 1500] loss 5.38, perplexity 217.98\n",
      "[Epoch 5 Batch 2000] loss 5.44, perplexity 231.11\n",
      "[Epoch 5 Batch 2500] loss 5.41, perplexity 223.90\n",
      "[Epoch 5 Batch 3000] loss 5.35, perplexity 210.16\n",
      "[Epoch 5 Batch 3500] loss 5.40, perplexity 220.36\n",
      "[Epoch 5 Batch 4000] loss 5.32, perplexity 204.31\n",
      "[Epoch 5 Batch 4500] loss 5.31, perplexity 202.15\n",
      "[Epoch 5 Batch 5000] loss 5.35, perplexity 211.48\n",
      "[Epoch 5 Batch 5500] loss 5.39, perplexity 219.55\n",
      "[Epoch 5] time cost 192.84s, validation loss 5.33, validation perplexity 206.42\n",
      "test loss 5.28, test perplexity 196.82\n",
      "[Epoch 6 Batch 500] loss 5.41, perplexity 223.18\n",
      "[Epoch 6 Batch 1000] loss 5.35, perplexity 211.43\n",
      "[Epoch 6 Batch 1500] loss 5.34, perplexity 209.38\n",
      "[Epoch 6 Batch 2000] loss 5.40, perplexity 221.63\n",
      "[Epoch 6 Batch 2500] loss 5.37, perplexity 215.18\n",
      "[Epoch 6 Batch 3000] loss 5.31, perplexity 201.71\n",
      "[Epoch 6 Batch 3500] loss 5.37, perplexity 214.04\n",
      "[Epoch 6 Batch 4000] loss 5.28, perplexity 197.31\n",
      "[Epoch 6 Batch 4500] loss 5.28, perplexity 195.42\n",
      "[Epoch 6 Batch 5000] loss 5.32, perplexity 204.74\n",
      "[Epoch 6 Batch 5500] loss 5.36, perplexity 212.00\n",
      "[Epoch 6] time cost 192.43s, validation loss 5.30, validation perplexity 200.79\n",
      "test loss 5.26, test perplexity 191.53\n",
      "[Epoch 7 Batch 500] loss 5.38, perplexity 216.00\n",
      "[Epoch 7 Batch 1000] loss 5.33, perplexity 206.10\n",
      "[Epoch 7 Batch 1500] loss 5.31, perplexity 202.63\n",
      "[Epoch 7 Batch 2000] loss 5.37, perplexity 214.45\n",
      "[Epoch 7 Batch 2500] loss 5.35, perplexity 209.69\n",
      "[Epoch 7 Batch 3000] loss 5.28, perplexity 197.10\n",
      "[Epoch 7 Batch 3500] loss 5.34, perplexity 208.08\n",
      "[Epoch 7 Batch 4000] loss 5.26, perplexity 191.82\n",
      "[Epoch 7 Batch 4500] loss 5.25, perplexity 190.87\n",
      "[Epoch 7 Batch 5000] loss 5.29, perplexity 199.03\n",
      "[Epoch 7 Batch 5500] loss 5.33, perplexity 206.92\n",
      "[Epoch 7] time cost 194.35s, validation loss 5.27, validation perplexity 193.97\n",
      "test loss 5.22, test perplexity 184.85\n",
      "[Epoch 8 Batch 500] loss 5.35, perplexity 209.65\n",
      "[Epoch 8 Batch 1000] loss 5.30, perplexity 200.83\n",
      "[Epoch 8 Batch 1500] loss 5.29, perplexity 197.42\n",
      "[Epoch 8 Batch 2000] loss 5.35, perplexity 209.56\n",
      "[Epoch 8 Batch 2500] loss 5.33, perplexity 205.42\n",
      "[Epoch 8 Batch 3000] loss 5.26, perplexity 192.11\n",
      "[Epoch 8 Batch 3500] loss 5.32, perplexity 203.84\n",
      "[Epoch 8 Batch 4000] loss 5.24, perplexity 187.97\n",
      "[Epoch 8 Batch 4500] loss 5.23, perplexity 186.45\n",
      "[Epoch 8 Batch 5000] loss 5.27, perplexity 195.24\n",
      "[Epoch 8 Batch 5500] loss 5.31, perplexity 202.02\n",
      "[Epoch 8] time cost 193.88s, validation loss 5.27, validation perplexity 194.45\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'args_lr' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-afbb149ccfa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_L\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best test loss %.2f, test perplexity %.2f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_L\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_L\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-4600b0ee2bda>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test loss %.2f, test perplexity %.2f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_L\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_L\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0margs_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs_lr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             trainer._init_optimizer('sgd',\n\u001b[1;32m     42\u001b[0m                                     {'learning_rate': args_lr,\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'args_lr' referenced before assignment"
     ]
    }
   ],
   "source": [
    "train()\n",
    "model.load_params(args_save, context)\n",
    "test_L = eval(test_data)\n",
    "print('Best test loss %.2f, test perplexity %.2f'%(test_L, math.exp(test_L)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
