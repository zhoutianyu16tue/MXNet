{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "\n",
    "from bucket_io import BucketSentenceIter, default_build_vocab\n",
    "\n",
    "data_dir = 'data'\n",
    "\n",
    "def Perplexity(label, pred):\n",
    "    \"\"\" Calculates prediction perplexity\n",
    "    Args:\n",
    "        label (mx.nd.array): labels array\n",
    "        pred (mx.nd.array): prediction array\n",
    "    Returns:\n",
    "        float: calculated perplexity\n",
    "    \"\"\"\n",
    "\n",
    "    # collapse the time, batch dimension\n",
    "    label = label.reshape((-1,))\n",
    "    pred = pred.reshape((-1, pred.shape[-1]))\n",
    "\n",
    "    loss = 0.\n",
    "    for i in range(pred.shape[0]):\n",
    "        loss += -np.log(max(1e-10, pred[i][int(label[i])]))\n",
    "    return np.exp(loss / label.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of dataset ==================\n",
      "bucket of len  10 : 19479 samples\n",
      "bucket of len  20 : 19336 samples\n",
      "bucket of len  30 : 12208 samples\n",
      "bucket of len  40 : 3962 samples\n",
      "bucket of len  50 : 845 samples\n",
      "bucket of len  60 : 160 samples\n",
      "Summary of dataset ==================\n",
      "bucket of len  10 : 1531 samples\n",
      "bucket of len  20 : 1518 samples\n",
      "bucket of len  30 : 980 samples\n",
      "bucket of len  40 : 322 samples\n",
      "bucket of len  50 : 65 samples\n",
      "bucket of len  60 : 10 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-22 16:29:24,340 Epoch[0] Batch [50]\tSpeed: 1477.57 samples/sec\tPerplexity=4431.916907\n",
      "2018-02-22 16:29:28,377 Epoch[0] Batch [100]\tSpeed: 1586.05 samples/sec\tPerplexity=681.280784\n",
      "2018-02-22 16:29:32,404 Epoch[0] Batch [150]\tSpeed: 1589.74 samples/sec\tPerplexity=523.909548\n",
      "2018-02-22 16:29:36,907 Epoch[0] Batch [200]\tSpeed: 1422.07 samples/sec\tPerplexity=440.165320\n",
      "2018-02-22 16:29:40,953 Epoch[0] Batch [250]\tSpeed: 1582.71 samples/sec\tPerplexity=385.760612\n",
      "2018-02-22 16:29:45,016 Epoch[0] Batch [300]\tSpeed: 1575.95 samples/sec\tPerplexity=365.302179\n",
      "2018-02-22 16:29:49,033 Epoch[0] Batch [350]\tSpeed: 1594.13 samples/sec\tPerplexity=343.126742\n",
      "2018-02-22 16:29:53,342 Epoch[0] Batch [400]\tSpeed: 1486.12 samples/sec\tPerplexity=326.637865\n",
      "2018-02-22 16:29:56,089 Epoch[0] Train-Perplexity=301.927836\n",
      "2018-02-22 16:29:56,091 Epoch[0] Time cost=36.191\n",
      "2018-02-22 16:29:58,630 Epoch[0] Validation-Perplexity=279.466159\n",
      "2018-02-22 16:30:02,871 Epoch[1] Batch [50]\tSpeed: 1526.67 samples/sec\tPerplexity=317.692976\n",
      "2018-02-22 16:30:06,886 Epoch[1] Batch [100]\tSpeed: 1594.90 samples/sec\tPerplexity=276.935171\n",
      "2018-02-22 16:30:10,942 Epoch[1] Batch [150]\tSpeed: 1578.78 samples/sec\tPerplexity=296.953441\n",
      "2018-02-22 16:30:15,400 Epoch[1] Batch [200]\tSpeed: 1436.00 samples/sec\tPerplexity=303.751829\n",
      "2018-02-22 16:30:19,530 Epoch[1] Batch [250]\tSpeed: 1550.33 samples/sec\tPerplexity=283.055493\n",
      "2018-02-22 16:30:23,648 Epoch[1] Batch [300]\tSpeed: 1554.99 samples/sec\tPerplexity=276.690508\n",
      "2018-02-22 16:30:27,687 Epoch[1] Batch [350]\tSpeed: 1585.07 samples/sec\tPerplexity=268.535283\n",
      "2018-02-22 16:30:32,009 Epoch[1] Batch [400]\tSpeed: 1481.67 samples/sec\tPerplexity=272.498147\n",
      "2018-02-22 16:30:34,693 Epoch[1] Train-Perplexity=257.725894\n",
      "2018-02-22 16:30:34,694 Epoch[1] Time cost=36.063\n",
      "2018-02-22 16:30:37,217 Epoch[1] Validation-Perplexity=240.629280\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "buckets = [10, 20, 30, 40, 50, 60]\n",
    "num_hidden = 200\n",
    "num_embed = 200\n",
    "num_lstm_layer = 2\n",
    "\n",
    "num_epoch = 2\n",
    "learning_rate = 0.01\n",
    "momentum = 0.0\n",
    "\n",
    "# Update count per available GPUs\n",
    "gpu_count = 1\n",
    "contexts = [mx.context.gpu(i) for i in range(gpu_count)]\n",
    "\n",
    "vocab = default_build_vocab(os.path.join(data_dir, 'ptb.train.txt'))\n",
    "\n",
    "init_h = [mx.io.DataDesc('LSTM_state', (num_lstm_layer, batch_size, num_hidden), layout='TNC')]\n",
    "init_c = [mx.io.DataDesc('LSTM_state_cell', (num_lstm_layer, batch_size, num_hidden), layout='TNC')]\n",
    "init_states = init_c + init_h\n",
    "\n",
    "data_train = BucketSentenceIter(os.path.join(data_dir, 'ptb.train.txt'),\n",
    "                                vocab, buckets, batch_size, init_states,\n",
    "                                time_major=True)\n",
    "data_val = BucketSentenceIter(os.path.join(data_dir, 'ptb.valid.txt'),\n",
    "                              vocab, buckets, batch_size, init_states,\n",
    "                              time_major=True)\n",
    "\n",
    "def sym_gen(seq_len):\n",
    "    \"\"\" Generates the MXNet symbol for the RNN\n",
    "    Args:\n",
    "        seq_len (int): input sequence length\n",
    "    Returns:\n",
    "        tuple: tuple containing symbol, data_names, label_names\n",
    "    \"\"\"\n",
    "    data = mx.sym.Variable('data')\n",
    "    label = mx.sym.Variable('softmax_label')\n",
    "    embed = mx.sym.Embedding(data=data, input_dim=len(vocab),\n",
    "                             output_dim=num_embed, name='embed')\n",
    "\n",
    "    # TODO(tofix)\n",
    "    # currently all the LSTM parameters are concatenated as\n",
    "    # a huge vector, and named '<name>_parameters'. By default\n",
    "    # mxnet initializer does not know how to initilize this\n",
    "    # guy because its name does not ends with _weight or _bias\n",
    "    # or anything familiar. Here we just use a temp workaround\n",
    "    # to create a variable and name it as LSTM_bias to get\n",
    "    # this demo running. Note by default bias is initialized\n",
    "    # as zeros, so this is not a good scheme. But calling it\n",
    "    # LSTM_weight is not good, as this is 1D vector, while\n",
    "    # the initialization scheme of a weight parameter needs\n",
    "    # at least two dimensions.\n",
    "    rnn_params = mx.sym.Variable('LSTM_bias')\n",
    "\n",
    "    # RNN cell takes input of shape (time, batch, feature)\n",
    "    rnn = mx.sym.RNN(data=embed, state_size=num_hidden,\n",
    "                     num_layers=num_lstm_layer, mode='lstm',\n",
    "                     name='LSTM',\n",
    "                     # The following params can be omitted\n",
    "                     # provided we do not need to apply the\n",
    "                     # workarounds mentioned above\n",
    "                     parameters=rnn_params)\n",
    "\n",
    "    # the RNN cell output is of shape (time, batch, dim)\n",
    "    # if we need the states and cell states in the last time\n",
    "    # step (e.g. when building encoder-decoder models), we\n",
    "    # can set state_outputs=True, and the RNN cell will have\n",
    "    # extra outputs: rnn['LSTM_output'], rnn['LSTM_state']\n",
    "    # and for LSTM, also rnn['LSTM_state_cell']\n",
    "\n",
    "    # now we collapse the time and batch dimension to do the\n",
    "    # final linear logistic regression prediction\n",
    "    hidden = mx.sym.Reshape(data=rnn, shape=(-1, num_hidden))\n",
    "\n",
    "    pred = mx.sym.FullyConnected(data=hidden, num_hidden=len(vocab),\n",
    "                                 name='pred')\n",
    "\n",
    "    # reshape to be of compatible shape as labels\n",
    "    pred_tm = mx.sym.Reshape(data=pred, shape=(seq_len, -1, len(vocab)))\n",
    "\n",
    "    sm = mx.sym.SoftmaxOutput(data=pred_tm, label=label, preserve_shape=True,\n",
    "                              name='softmax')\n",
    "\n",
    "    data_names = ['data', 'LSTM_state', 'LSTM_state_cell']\n",
    "    label_names = ['softmax_label']\n",
    "\n",
    "    return sm, data_names, label_names\n",
    "\n",
    "if len(buckets) == 1:\n",
    "    mod = mx.mod.Module(*sym_gen(buckets[0]), context=contexts)\n",
    "else:\n",
    "    mod = mx.mod.BucketingModule(sym_gen,\n",
    "                                 default_bucket_key=data_train.default_bucket_key,\n",
    "                                 context=contexts)\n",
    "\n",
    "import logging\n",
    "\n",
    "head = '%(asctime)-15s %(message)s'\n",
    "logging.basicConfig(level=logging.DEBUG, format=head)\n",
    "\n",
    "mod.fit(data_train, eval_data=data_val, num_epoch=num_epoch,\n",
    "        eval_metric=mx.metric.np(Perplexity),\n",
    "        batch_end_callback=mx.callback.Speedometer(batch_size, 50),\n",
    "        initializer=mx.init.Xavier(factor_type=\"in\", magnitude=2.34),\n",
    "        optimizer='sgd',\n",
    "        optimizer_params={'learning_rate': learning_rate,\n",
    "                          'momentum': momentum, 'wd': 0.00001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of dataset ==================\n",
      "bucket of len  10 : 1726 samples\n",
      "bucket of len  20 : 1699 samples\n",
      "bucket of len  30 : 1108 samples\n",
      "bucket of len  40 : 355 samples\n",
      "bucket of len  50 : 69 samples\n",
      "bucket of len  60 : 14 samples\n"
     ]
    }
   ],
   "source": [
    "data_test = BucketSentenceIter(os.path.join(data_dir, 'ptb.test.txt'),\n",
    "                              vocab, buckets, batch_size, init_states,\n",
    "                              time_major=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvalMetric: {'Perplexity': 226.84756651445085}\n"
     ]
    }
   ],
   "source": [
    "perplexity = mx.metric.np(Perplexity)\n",
    "mod.score(data_test, perplexity)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mod' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fe961dd98716>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mod' is not defined"
     ]
    }
   ],
   "source": [
    "test_pred = mod.predict(data_test)\n",
    "test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(710L, 128L, 9959L)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   33.,    27.,    70.,   989.,  5436.,     0.,     0.,     0.,\n",
       "           0.,     0.])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_test.data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[' ']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
