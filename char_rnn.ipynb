{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "\n",
    "from bucket_io import BucketSentenceIter, default_build_vocab\n",
    "\n",
    "data_dir = 'data'\n",
    "\n",
    "def Perplexity(label, pred):\n",
    "    \"\"\" Calculates prediction perplexity\n",
    "    Args:\n",
    "        label (mx.nd.array): labels array\n",
    "        pred (mx.nd.array): prediction array\n",
    "    Returns:\n",
    "        float: calculated perplexity\n",
    "    \"\"\"\n",
    "    # collapse the time, batch dimension\n",
    "    label = label.reshape((-1,))\n",
    "    pred = pred.reshape((-1, pred.shape[-1]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    loss = 0.\n",
    "    for i in range(pred.shape[0]):\n",
    "        loss += -np.log(max(1e-10, pred[i][int(label[i])]))\n",
    "    return np.exp(loss / label.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "buckets = [11, 21, 31, 41]\n",
    "num_hidden = 200\n",
    "num_embed = 200\n",
    "num_lstm_layer = 2\n",
    "\n",
    "num_epoch = 2\n",
    "learning_rate = 0.01\n",
    "momentum = 0.0\n",
    "\n",
    "# Update count per available GPUs\n",
    "gpu_count = 1\n",
    "contexts = [mx.context.gpu(i) for i in range(gpu_count)]\n",
    "# contexts = mx.cpu()\n",
    "vocab = default_build_vocab(os.path.join(data_dir, 'path_train.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.append(0)\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of dataset ==================\n",
      "bucket of len  11 : 7796 samples\n",
      "bucket of len  21 : 10202 samples\n",
      "bucket of len  31 : 3127 samples\n",
      "bucket of len  41 : 203 samples\n",
      "Summary of dataset ==================\n",
      "bucket of len  11 : 1039 samples\n",
      "bucket of len  21 : 1361 samples\n",
      "bucket of len  31 : 417 samples\n",
      "bucket of len  41 : 27 samples\n"
     ]
    }
   ],
   "source": [
    "init_h = [mx.io.DataDesc('LSTM_state', (num_lstm_layer, batch_size, num_hidden), layout='TNC')]\n",
    "init_c = [mx.io.DataDesc('LSTM_state_cell', (num_lstm_layer, batch_size, num_hidden), layout='TNC')]\n",
    "init_states = init_c + init_h\n",
    "\n",
    "data_train = BucketSentenceIter(os.path.join(data_dir, 'path_train.txt'),\n",
    "                                vocab, buckets, batch_size, init_states,\n",
    "                                time_major=True)\n",
    "data_val = BucketSentenceIter(os.path.join(data_dir, 'path_val.txt'),\n",
    "                              vocab, buckets, batch_size, init_states,\n",
    "                              time_major=True)\n",
    "\n",
    "def sym_gen(seq_len):\n",
    "    \"\"\" Generates the MXNet symbol for the RNN\n",
    "    Args:\n",
    "        seq_len (int): input sequence length\n",
    "    Returns:\n",
    "        tuple: tuple containing symbol, data_names, label_names\n",
    "    \"\"\"\n",
    "    data = mx.sym.Variable('data')\n",
    "    label = mx.sym.Variable('softmax_label')\n",
    "    embed = mx.sym.Embedding(data=data, input_dim=len(vocab),\n",
    "                             output_dim=num_embed, name='embed')\n",
    "\n",
    "    # TODO(tofix)\n",
    "    # currently all the LSTM parameters are concatenated as\n",
    "    # a huge vector, and named '<name>_parameters'. By default\n",
    "    # mxnet initializer does not know how to initilize this\n",
    "    # guy because its name does not ends with _weight or _bias\n",
    "    # or anything familiar. Here we just use a temp workaround\n",
    "    # to create a variable and name it as LSTM_bias to get\n",
    "    # this demo running. Note by default bias is initialized\n",
    "    # as zeros, so this is not a good scheme. But calling it\n",
    "    # LSTM_weight is not good, as this is 1D vector, while\n",
    "    # the initialization scheme of a weight parameter needs\n",
    "    # at least two dimensions.\n",
    "    rnn_params = mx.sym.Variable('LSTM_bias')\n",
    "\n",
    "    # RNN cell takes input of shape (time, batch, feature)\n",
    "    rnn = mx.sym.RNN(data=embed, state_size=num_hidden,\n",
    "                     num_layers=num_lstm_layer, mode='lstm',\n",
    "                     name='LSTM',\n",
    "                     # The following params can be omitted\n",
    "                     # provided we do not need to apply the\n",
    "                     # workarounds mentioned above\n",
    "                     parameters=rnn_params)\n",
    "\n",
    "    # the RNN cell output is of shape (time, batch, dim)\n",
    "    # if we need the states and cell states in the last time\n",
    "    # step (e.g. when building encoder-decoder models), we\n",
    "    # can set state_outputs=True, and the RNN cell will have\n",
    "    # extra outputs: rnn['LSTM_output'], rnn['LSTM_state']\n",
    "    # and for LSTM, also rnn['LSTM_state_cell']\n",
    "\n",
    "    # now we collapse the time and batch dimension to do the\n",
    "    # final linear logistic regression prediction\n",
    "    hidden = mx.sym.Reshape(data=rnn, shape=(-1, num_hidden))\n",
    "\n",
    "    pred = mx.sym.FullyConnected(data=hidden, num_hidden=len(vocab),\n",
    "                                 name='pred')\n",
    "\n",
    "    # reshape to be of compatible shape as labels\n",
    "    pred_tm = mx.sym.Reshape(data=pred, shape=(seq_len, -1, len(vocab)))\n",
    "\n",
    "    sm = mx.sym.SoftmaxOutput(data=pred_tm, label=label, preserve_shape=True,\n",
    "                              name='softmax')\n",
    "\n",
    "    data_names = ['data', 'LSTM_state', 'LSTM_state_cell']\n",
    "    label_names = ['softmax_label']\n",
    "\n",
    "    return sm, data_names, label_names\n",
    "\n",
    "if len(buckets) == 1:\n",
    "    mod = mx.mod.Module(*sym_gen(buckets[0]), context=contexts)\n",
    "else:\n",
    "    mod = mx.mod.BucketingModule(sym_gen,\n",
    "                                 default_bucket_key=data_train.default_bucket_key,\n",
    "                                 context=contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-23 14:13:05,055 Epoch[0] Batch [50]\tSpeed: 7655.35 samples/sec\tPerplexity=123.461754\n",
      "2018-02-23 14:13:05,380 Epoch[0] Batch [100]\tSpeed: 9901.76 samples/sec\tPerplexity=62.459987\n",
      "2018-02-23 14:13:05,758 Epoch[0] Batch [150]\tSpeed: 8509.26 samples/sec\tPerplexity=60.688413\n",
      "2018-02-23 14:13:06,135 Epoch[0] Batch [200]\tSpeed: 8511.43 samples/sec\tPerplexity=54.228900\n",
      "2018-02-23 14:13:06,474 Epoch[0] Batch [250]\tSpeed: 9492.28 samples/sec\tPerplexity=50.319478\n",
      "2018-02-23 14:13:06,845 Epoch[0] Batch [300]\tSpeed: 8684.47 samples/sec\tPerplexity=56.297255\n",
      "2018-02-23 14:13:07,039 Epoch[0] Train-Perplexity=51.318953\n",
      "2018-02-23 14:13:07,040 Epoch[0] Time cost=2.434\n",
      "2018-02-23 14:13:07,313 Epoch[0] Validation-Perplexity=55.509679\n",
      "2018-02-23 14:13:07,684 Epoch[1] Batch [50]\tSpeed: 8804.51 samples/sec\tPerplexity=57.895443\n",
      "2018-02-23 14:13:08,045 Epoch[1] Batch [100]\tSpeed: 8918.52 samples/sec\tPerplexity=51.374932\n",
      "2018-02-23 14:13:08,430 Epoch[1] Batch [150]\tSpeed: 8349.27 samples/sec\tPerplexity=55.834942\n",
      "2018-02-23 14:13:08,794 Epoch[1] Batch [200]\tSpeed: 8827.54 samples/sec\tPerplexity=53.648396\n",
      "2018-02-23 14:13:09,135 Epoch[1] Batch [250]\tSpeed: 9434.94 samples/sec\tPerplexity=48.285086\n",
      "2018-02-23 14:13:09,505 Epoch[1] Batch [300]\tSpeed: 8690.55 samples/sec\tPerplexity=54.299721\n",
      "2018-02-23 14:13:09,703 Epoch[1] Train-Perplexity=49.911605\n",
      "2018-02-23 14:13:09,705 Epoch[1] Time cost=2.390\n",
      "2018-02-23 14:13:09,982 Epoch[1] Validation-Perplexity=54.684362\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "head = '%(asctime)-15s %(message)s'\n",
    "logging.basicConfig(level=logging.DEBUG, format=head)\n",
    "\n",
    "mod.fit(data_train, eval_data=data_val, num_epoch=num_epoch,\n",
    "        eval_metric=mx.metric.np(Perplexity),\n",
    "        batch_end_callback=mx.callback.Speedometer(batch_size, 50),\n",
    "        initializer=mx.init.Xavier(factor_type=\"in\", magnitude=2.34),\n",
    "        optimizer='sgd',\n",
    "        optimizer_params={'learning_rate': learning_rate,\n",
    "                          'momentum': momentum, 'wd': 0.00001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of dataset ==================\n",
      "bucket of len  11 : 1559 samples\n",
      "bucket of len  21 : 2040 samples\n",
      "bucket of len  31 : 625 samples\n",
      "bucket of len  41 : 41 samples\n"
     ]
    }
   ],
   "source": [
    "data_test = BucketSentenceIter(os.path.join(data_dir, 'path_test.txt'),\n",
    "                              vocab, buckets, batch_size, init_states,\n",
    "                              time_major=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvalMetric: {'Perplexity': 55.544038522751499}\n"
     ]
    }
   ],
   "source": [
    "perplexity = mx.metric.np(Perplexity)\n",
    "mod.score(data_test, perplexity)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
